# ğŸ³ Recipe RAG CLI (v1)

A lightweight Retrieval-Augmented Generation (RAG) system for answering questions about recipes using embeddings, ChromaDB, and an LLM â€” all from the command line.

---

## ğŸ“Œ Overview

This project implements a minimal, reproducible RAG pipeline:

- ğŸ“„ Markdown recipes as source data  
- âœ‚ï¸ Chunking + JSONL storage  
- ğŸ§  Embeddings stored in Chroma  
- ğŸ” Top-k semantic retrieval  
- ğŸ¤– LLM generation with sources  
- ğŸ’» Simple CLI interface  

---

# ğŸ— Architecture

## Data Flow Diagram

### ğŸ§± Ingest / Index (One-Time or Occasional)

```
recipes.md
    â†“
chunks.jsonl
    â†“ (embed)
Chroma DB (persist/recipes_v1)
```

**Process:**

1. Parse `recipes.md`
2. Split into chunks
3. Save chunks to `chunks.jsonl`
4. Generate embeddings
5. Store vectors in Chroma (persistent)

---

### ğŸ’¬ Run (Per Question)

```
CLI question
    â†“ (embed query)
Chroma top-k
    â†“ (select best-1)
LLM
    â†“
Answer + Sources footer
```

**Process:**

1. User asks a question via CLI  
2. Embed the query  
3. Retrieve top-k relevant chunks  
4. Select best match  
5. Send context to LLM  
6. Return answer with source attribution  

---

# ğŸ“ Project Structure (v1)

```
.
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ recipes.md
â”‚   â”œâ”€â”€ chunks.jsonl
â”‚   â””â”€â”€ chroma/                # Chroma persist directory
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ parse/                 # Optional (for reproducibility)
â”‚   â”œâ”€â”€ index/
â”‚   â”‚   â””â”€â”€ build_index.py
â”‚   â”œâ”€â”€ rag/
â”‚   â”‚   â”œâ”€â”€ retrieve.py
â”‚   â”‚   â”œâ”€â”€ prompt.py
â”‚   â”‚   â””â”€â”€ generate.py
â”‚   â””â”€â”€ cli.py
â”‚
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ v1.yaml                # paths, embedding model, k, LLM model
â”‚
â”œâ”€â”€ logs/
â”‚   â””â”€â”€ queries.jsonl
â”‚
â””â”€â”€ docs/
    â””â”€â”€ architecture_v1.md     # Add diagram image later
```

---

# âš™ï¸ Configuration

Example `configs/v1.yaml`:

```yaml
paths:
  recipes: data/recipes.md
  chunks: data/chunks.jsonl
  chroma: data/chroma/

embedding:
  model: nomic-embed-text
  k: 5

llm:
  model: llama3
```

---

# ğŸš€ Usage

### 1ï¸âƒ£ Build the Index

```bash
python src/index/build_index.py
```

### 2ï¸âƒ£ Ask a Question

```bash
python src/cli.py "How do I make fluffy pancakes?"
```

---

# ğŸ§  System Components

| Layer        | Responsibility |
|-------------|----------------|
| Data Layer  | Markdown â†’ JSONL chunks |
| Vector Layer | Embeddings stored in Chroma |
| Retrieval Layer | Top-k semantic search |
| Generation Layer | Prompt + context â†’ LLM |
| Interface | CLI |

---

# ğŸ“ Logging

All queries are logged for evaluation and debugging:

```
logs/queries.jsonl
```

Each entry may include:

- User question  
- Retrieved chunks  
- Selected context  
- Final answer  

---

# ğŸ”® Future Improvements

- Evaluation pipeline  
- Reranking step  
- Streaming responses  
- Web UI  
- Structured source citations  
- Multi-document support  

---

# ğŸ· Version

**v1 â€” Minimal, reproducible, CLI-based RAG system**

---

Built for clarity, iteration, and extensibility.